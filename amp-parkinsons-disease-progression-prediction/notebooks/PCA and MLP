import time
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import KNNImputer, SimpleImputer
from scipy import linalg
import collections
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras import backend as kb

df_train_clinical = pd.read_csv('C:/Users/matt.DESKTOP-MPIK5VI/Desktop/ECE 539/Project/Data/train_clinical_data.csv')
df_train_peptides = pd.read_csv('C:/Users/matt.DESKTOP-MPIK5VI/Desktop/ECE 539/Project/Data/train_peptides.csv')
df_train_proteins = pd.read_csv('C:/Users/matt.DESKTOP-MPIK5VI/Desktop/ECE 539/Project/Data/train_proteins.csv')

# In[1]


print(f'Unique Clinical Data patient count: {df_train_clinical["patient_id"].nunique()}')
print('-------------------------------------------------------')
print('Null Values Found in Clinical Data:')
for col in df_train_clinical.columns:
    print(f'Null values found in {col}: {df_train_clinical[col].isna().sum()}')

print(f'\nUnique Peptide Data patient count: {df_train_peptides["patient_id"].nunique()}')
print(f'Unique Peptide count: {df_train_peptides["Peptide"].nunique()}')
print(f'Unique Protein count: {df_train_peptides["UniProt"].nunique()}')
print('-------------------------------------------------------')
print('Null Values Found in Peptide Data:')
for col in df_train_peptides.columns:
    print(f'Null values found in {col}: {df_train_peptides[col].isna().sum()}')
    
print(f'\nUnique Protein Data patient count: {df_train_proteins["patient_id"].nunique()}')
print(f'Unique Protein count: {df_train_proteins["UniProt"].nunique()}')
print('-------------------------------------------------------')
print('Null Values Found in Protein Data:')
for col in df_train_proteins.columns:
    print(f'Null values found in {col}: {df_train_proteins[col].isna().sum()}')
    
print('\nInput Data Shapes:')
print(f'Clinical Data: {df_train_clinical.shape}')
print(f'Peptide Data: {df_train_peptides.shape}')
print(f'Protein Data: {df_train_proteins.shape}')

# In[2]


def SMAPE(y_true, y_pred):
    """
    Computes the Symmetric Mean Absolute Percentage Error (SMAPE) loss between y_true and y_pred.
    """
    epsilon = 0.1
    summ = kb.maximum(kb.abs(y_true) + kb.abs(y_pred) + epsilon, 0.5+epsilon)
    smape = kb.mean(kb.abs(y_pred - y_true) / summ * 2)
    return smape*100

# In[3]


# Restructure to have UniProt columns, visit_id rows and NPX as values
protein_pivot = df_train_proteins.pivot(index = 'visit_id', columns=['UniProt'], values='NPX')
# print(protein_pivot.head(10))

# # Check if NaN's in protein data are due to missing peptide abundances 
# dict_visit_uniprot_pep = collections.defaultdict(set)
# for visit, uniprot in zip(df_train_peptides['visit_id'].values, df_train_peptides['UniProt'].values):
#     dict_visit_uniprot_pep[visit].add(uniprot)

# dict_visit_uniprot_prot = collections.defaultdict(set)
# for visit, uniprot in zip(df_train_proteins['visit_id'].values, df_train_proteins['UniProt'].values):
#     dict_visit_uniprot_prot[visit].add(uniprot)

# # If true, then each visit contains the same set of proteins in both train_peptides and train_proteins
# print(dict_visit_uniprot_pep==dict_visit_uniprot_prot)
# # Cannot impute missing NPX values from PeptideAbundance

# In[Make new dataframe]

df_updrs_protein = pd.merge(df_train_clinical[['visit_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']], 
                            df_train_proteins[['visit_id', 'UniProt', 'NPX']], on='visit_id')
protein_pivot = df_updrs_protein.pivot(index='visit_id', columns=['UniProt'], values='NPX')

print('Missing visit_ids in clinical data:', set(df_train_proteins['visit_id']).difference(set(df_train_clinical['visit_id'])))

peptide_pivot = df_train_peptides.pivot(index='visit_id', columns=['Peptide'], values='PeptideAbundance')

df_updrs_peptide = pd.merge(df_train_clinical[['visit_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']], 
                            df_train_peptides[['visit_id', 'Peptide', 'PeptideAbundance']], on='visit_id')
peptide_pivot = df_updrs_peptide.pivot(index='visit_id', columns=['Peptide'], values='PeptideAbundance')

df_indices = peptide_pivot.index
df_prot_columns = protein_pivot.columns
df_pep_columns = peptide_pivot.columns

# In[4]


# Imputation

# KNN
imputer_prot_knn = KNNImputer(missing_values=np.nan)
print('NaN before knn imputation:', protein_pivot.isna().sum().sum())
proteins_knn = imputer_prot_knn.fit_transform(protein_pivot)
print('NaN after knn imputation:', np.sum(np.isnan(proteins_knn)))

# Mean
imputer_prot_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before mean imputation:', protein_pivot.isna().sum().sum())
proteins_mean = imputer_prot_mean.fit_transform(protein_pivot)
print('NaN after mean imputation:', np.sum(np.isnan(proteins_mean)))

# Zero
print('NaN before 0 imputation:', protein_pivot.isna().sum().sum())
proteins_zero = protein_pivot.fillna(0, inplace=False)
print('NaN after 0 imputation:', proteins_zero.isna().sum().sum())

# In[4.5] Prep for Linear Regression
global t_reg
t_reg = []
t0 = time.time()
def visit_id_to_patient_month(visit_id):
    '''
    Takes an array of visit_ids, converts to list of lists of shape=(len(visit_id),2)
    where index 0 contains visit_ids and index 1 contains visit_months
    '''
    return [np.array(x.split('_'), dtype='int32') for x in visit_id]

def patient_month_to_visit_id(patient, month):
    return '_'.join(np.array([patient,month], dtype='str'))

# Store visit_month and NPX pairs for each protein for each patient in dictionary
# (patient_id, UniProt):[(visit_month, NPX), ...]
dict_patient_NPX = {key:[g['visit_month'].values, g['NPX'].values] 
                    for key,g in df_train_proteins.groupby(['patient_id', 'UniProt'])}

# In[4.75]


# Create an imputer that predicts missing peptide/protein values via linear regression
# of a patients visits that successfully reported the PeptideAbundance/NPX of the 
# peptide/protein of interest.

def linRegression(X, Y, month):
    '''
    Performs linear regression via least squares to estimate the coefficients for
    the line of best fit between X and Y data. The coefficients are then used to
    estimate the value associated with that of the month parameter.

    Parameters
    ----------
    X : array
        X-values or independent variables to use for linear regression
        Shape=(n,)
    Y : array
        Y values or dependent variable to use for linear regression
        Shape=(n,)
    month : int
        The value of the independent variable for which the corresponding predicted
        dependent value is desired.

    Returns
    -------
    float
        The predicted value calculated via month and the linear regression coefficients

    '''
    t_reg.append(time.time())
    #If only one other visit has an NPX value, return that value
    if len(Y) == 1:
        return Y[0]
    
    # prep data
    poly = np.zeros((len(X),2))
    for i,num in enumerate(X):
        for j in range(2):
            poly[i][j] = num**(j)       
    
    # Calculate coefficients and return predicted NPX
    poly_t = np.transpose(poly)
    coeffs = ((linalg.inv(poly_t.dot(poly)).dot(poly_t))).dot(Y)
    res = np.array([1,month]).dot(coeffs)
    t_reg.append(time.time())
    return np.array([1,month]).dot(coeffs)


# In[linear regression imputation]

# Implement imputation via linear regression
def LR_Imputer(data, data_dict):
    '''
    Applies linear regression as a strategy for imputation of data with NaN values

    Parameters
    ----------
    data : DataFrame
        Indices: visit_id
        Columns: Either peptide names or UniProt ids
    data_dict : dict
        Keys: (patient_id, UniProt) tuple
        Values: [visit_month, NPX or PeptideAbundance] list of np.arrays of int32s

    Returns
    -------
    New DataFrame object that is result of imputation

    '''
    print('NaN before linear regression imputation:', data.isna().sum().sum())
    
    data_copy = data.copy(deep=True)
    # Iterate through all columns (protein/peptide) in data
    for name, content in data_copy.items():
        # If no NaN's for this protein, jump to next
        if content.isna().sum() == 0:
            continue
        keys = visit_id_to_patient_month(content.index)
        skip_patient = None
        
        # Iterate over each row (visit_id) for protein
        for idx,npx in enumerate(content):
            patient = keys[idx][0]
            
            if np.isnan(npx) and (patient != skip_patient):
                if (patient,name) not in data_dict.keys():
                    skip_patient = patient
                    continue
                X, Y = data_dict[(patient, name)]
                visit_id = patient_month_to_visit_id(patient, keys[idx][1])
                pred_npx = linRegression(X, Y, keys[idx][1])
                data_copy.loc[visit_id, name] = pred_npx
    
    print('NaN after linear regression imputation:', data_copy.isna().sum().sum())
    return data_copy

proteins_lr = LR_Imputer(protein_pivot, dict_patient_NPX)

t1 = time.time()
print('TIME TIME TIME TIME TIME TIME TIME TIME TIME TIME TIME TIME\n', t1-t0)
print(np.sum([t_reg[2*i+1]-t_reg[2*i] for i in range(int(len(t_reg)/2))]))
# Assess imputation
has_null = 0
for col in proteins_lr:
    if proteins_lr[col].isna().sum() != 0:
        has_null+=1
print(f'Percent proteins with >= one null value after imputation: {100*has_null/len(proteins_lr.columns)}')
has_null = 0
for idx in proteins_lr.index:
    if proteins_lr.loc[idx].isna().sum() != 0:
        has_null+=1
print(f'Percent visits with >= one null value after imputation: {100*has_null/len(proteins_lr.index)}')

# In[Assess imputation]

# Still has null values, can set to 0, use full column means, or knn
# KNN
imputer_prot_lr_knn = KNNImputer(missing_values=np.nan) # default: k=5
print('NaN before knn imputation:', proteins_lr.isna().sum().sum())
proteins_lr_knn = imputer_prot_knn.fit_transform(proteins_lr)
print('NaN after knn imputation:', np.sum(np.isnan(proteins_lr_knn)))

# Mean
imputer_prot_lr_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before mean imputation:', proteins_lr.isna().sum().sum())
proteins_lr_mean = imputer_prot_mean.fit_transform(proteins_lr)
print('NaN after mean imputation:', np.sum(np.isnan(proteins_lr_mean)))

# Zero
print('NaN before 0 imputation:', proteins_lr.isna().sum().sum())
proteins_lr_zero = proteins_lr.fillna(0, inplace=False)
print('NaN after 0 imputation:', proteins_lr_zero.isna().sum().sum())
# In[5]

# Scale Protein NPX values for all imputation methods
scaler_proteins_knn = StandardScaler()
proteins_knn = pd.DataFrame(scaler_proteins_knn.fit_transform(proteins_knn), columns=df_prot_columns, index=df_indices)

scaler_proteins_mean = StandardScaler()
proteins_mean = pd.DataFrame(scaler_proteins_mean.fit_transform(proteins_mean), columns=df_prot_columns, index=df_indices)

scaler_proteins_zero = StandardScaler()
proteins_zero = pd.DataFrame(scaler_proteins_zero.fit_transform(proteins_zero), columns=df_prot_columns, index=df_indices)

scaler_proteins_lr_knn = StandardScaler()
proteins_lr_knn = pd.DataFrame(scaler_proteins_lr_knn.fit_transform(proteins_lr_knn), columns=df_prot_columns, index=df_indices)

scaler_proteins_lr_mean = StandardScaler()
proteins_lr_mean = pd.DataFrame(scaler_proteins_lr_mean.fit_transform(proteins_lr_mean), columns=df_prot_columns, index=df_indices)

scaler_proteins_lr_zero = StandardScaler()
proteins_lr_zero = pd.DataFrame(scaler_proteins_lr_zero.fit_transform(proteins_lr_zero), columns=df_prot_columns, index=df_indices)

# In[6]


# PCA on proteins for all imputation methods
def run_PCA(data, names, explained_var):
    '''
    Performs PCA on each array in data.

    Parameters
    ----------
    data : list of numpy arrays
        Datasets containing protein or peptide abundance data at each patient visit
    names : list of strings
        Names of each dataset, index should match that of the corresponding dataset's in data
    explained_var : float
        Between zero and one, desired ratio of explained variance. The PCA objects
        use this to determine how many principal components to compute.

    Returns
    -------
    dict
        keys : int
            Names of original data from the argument names
        values : list
            Index 0 contains the PCA object fit to the data
            Index 1 contains the transformed data as a numpy array

    '''
    pca_objs = []
    pca_res = []
    for array in data:
        pca_objs.append(PCA(explained_var, svd_solver='full'))
        pca_res.append(pca_objs[-1].fit_transform(array))

    return {name:(obj,res) for name,obj,res in zip(names,pca_objs,pca_res)}

# Prep data for PCA function
scaled_prot_data = [proteins_knn, proteins_mean, proteins_zero,
               proteins_lr_knn, proteins_lr_mean, proteins_lr_zero]
scaled_names = ['KNN', 'Mean', 'Zero Fill', 'LR + KNN', 'LR + Mean', 'LR + Zero Fill']

# Perform PCA
pca_prot_data = run_PCA(scaled_prot_data, scaled_names, 0.85)


# In[7]

def Visualize_PCA_Results(data, explained_var, sup_title=''):
    '''
    Visualize the results of PCA on six different types of data. Figure contains six
    scatter plots and one bar graph along the bottom. Each scatter plot shows the 
    first two Principal components of the specified PCA data. The bar graph plots the
    explained variance ratios of the first 10 PC's for each of the 6 datasets. 
    The function also prints the number of PC's required to reach the desired explained
    variance ratio.

    Parameters
    ----------
    data : dict
        Output of run_PCA function:
        keys : int
            Dataset names
        values : list
            Index 0 contains the PCA object fit to the data
            Index 1 contains the transformed data as a numpy array
    explained_var : float
        Between zero and one, desired ratio of explained variance. The PCA objects
        use this to determine how many principal components to compute.
    sup_title : str
        Allows customization of the figure's title to include the name of the data.
        For our use, either 'Protein' or 'Peptide'

    Returns
    -------
    None.

    '''
    width = 0.15
    sing_values = 10
    x_vals = np.arange(sing_values)
    titles = data.keys()
    
    fig,axes = plt.subplots(ncols=3, nrows=3, figsize=(10,10), layout='tight')
    fig.suptitle(f'PCA After {sup_title} Data Imputation')
    
    # Make large subplot on bottom row for combined bar graph
    gs = axes[2,0].get_gridspec()
    for ax in axes[2,:]:
        ax.remove()
    ax_bar = fig.add_subplot(gs[2,:])
    
    for i,title in enumerate(titles):
        # Build bar graph
        ax_bar.bar(x_vals-0.425+i*width, data[title][0].explained_variance_ratio_[:sing_values], width, label=title)
        ax_bar.set_xlabel('Principle Components')
        ax_bar.set_ylabel('Explained Variance Ratio')
        ax_bar.set_xticks(x_vals)
        ax_bar.set_xticklabels(['PC '+str(x+1) for x in x_vals])
        ax_bar.legend()
        
        # Add scatter plot of first two principal components
        cur_ax = axes[i//3, i%3]
        cur_ax.scatter(data[title][1][:,0], data[title][1][:,1])
        cur_ax.set_xlabel('PC 1')
        cur_ax.set_ylabel('PC 2')
        cur_ax.set_title(title)
    
    plt.tight_layout()
    
    print(f'PC\'s necessary to explain {explained_var*100}% of variance:')
    for title in titles:
        print(f'{title} Imputation: {data[title][0].n_components_}')

Visualize_PCA_Results(pca_prot_data, 0.85, 'Protein')
    
# In[8]

# Run above for peptides
# Imputation
imputer_pep_knn = KNNImputer(missing_values=np.nan)
print('NaN before KNN imputation (peptides): ', peptide_pivot.isna().sum().sum())
peptides_knn = imputer_pep_knn.fit_transform(peptide_pivot)
print('NaN after KNN imputation (peptides): ', np.sum(np.isnan(peptides_knn)))

imputer_pep_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before mean imputation (peptides): ', peptide_pivot.isna().sum().sum())
peptides_mean = imputer_pep_mean.fit_transform(peptide_pivot)
print('NaN after mean imputation (peptides): ', np.sum(np.isnan(peptides_mean)))

print('NaN before zero fill imputation (peptides): ', peptide_pivot.isna().sum().sum())
peptides_zero = peptide_pivot.fillna(0, inplace=False)
print('NaN after zero fill imputation (peptides): ', peptides_zero.isna().sum().sum())

dict_patient_Abund = {key:[g['visit_month'].values, g['PeptideAbundance'].values]
                      for key, g in df_train_peptides.groupby(['patient_id', 'Peptide'])}

peptides_lr = LR_Imputer(peptide_pivot, dict_patient_Abund)

# Assess imputation
has_null = 0
for col in peptides_lr:
    if peptides_lr[col].isna().sum() != 0:
        has_null+=1
print(f'Percent peptides with >= one null value after imputation: {100*has_null/len(peptides_lr.columns)}')
has_null = 0
for idx in peptides_lr.index:
    if peptides_lr.loc[idx].isna().sum() != 0:
        has_null+=1
print(f'Percent visits with >= one null value after imputation: {100*has_null/len(peptides_lr.index)}')

# Still has null values, can set to 0, use full column means, or knn
imputer_pep_lr_knn = KNNImputer(missing_values=np.nan)
print('NaN before KNN Imputation: ', peptide_pivot.isna().sum().sum())
peptides_lr_knn = imputer_pep_lr_knn.fit_transform(peptides_lr)
print('NaN after KNN Imputation: ', np.sum(np.isnan(peptides_lr_knn)))

imputer_pep_lr_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before Mean Imputation: ', peptide_pivot.isna().sum().sum())
peptides_lr_mean = imputer_pep_lr_mean.fit_transform(peptides_lr)
print('NaN after Mean Imputation: ', np.sum(np.isnan(peptides_lr_mean)))

print('NaN before Zero Fill Imputation: ', peptide_pivot.isna().sum().sum())
peptides_lr_zero = peptide_pivot.fillna(0, inplace=False)
print('NaN after Zero Fill Imputation: ', peptides_lr_zero.isna().sum().sum())


# Scaling
scaler_pep_knn = StandardScaler()
peptides_knn = pd.DataFrame(scaler_pep_knn.fit_transform(peptides_knn), columns=df_pep_columns, index=df_indices)

scaler_pep_mean = StandardScaler()
peptides_mean = pd.DataFrame(scaler_pep_mean.fit_transform(peptides_mean), columns=df_pep_columns, index=df_indices)

scaler_pep_zero = StandardScaler()
peptides_zero = pd.DataFrame(scaler_pep_zero.fit_transform(peptides_zero), columns=df_pep_columns, index=df_indices)

scaler_pep_lr_knn = StandardScaler()
peptides_lr_knn = pd.DataFrame(scaler_pep_lr_knn.fit_transform(peptides_lr_knn), columns=df_pep_columns, index=df_indices)

scaler_pep_lr_mean = StandardScaler()
peptides_lr_mean = pd.DataFrame(scaler_pep_lr_mean.fit_transform(peptides_lr_mean), columns=df_pep_columns, index=df_indices)

scaler_pep_lr_zero = StandardScaler()
peptides_lr_zero = pd.DataFrame(scaler_pep_lr_zero.fit_transform(peptides_lr_zero), columns=df_pep_columns, index=df_indices)

# In[peptide pca]
# PCA
scaled_pep_data = [peptides_knn, peptides_mean, peptides_zero,
                   peptides_lr_knn, peptides_lr_mean, peptides_lr_zero]

pca_pep_data = run_PCA(scaled_pep_data, scaled_names, 0.85)

# Plot PCA data
Visualize_PCA_Results(pca_pep_data, 0.85, 'Peptide')

# In[9]


############# Keep a column seperate during PCA!!!! ######################

# featurizer = ColumnTransformer([('pca', PCA(n_components=1), ['c1', 'c2']),
# ...                                 ('preserve', 'passthrough', ['c_to_preserve'])])
# >>> featurizer.fit_transform(df)
# array([[ 4.03887361, -5.        ],
#        [ 1.3462912 , -3.        ],
#        [-1.3462912 ,  6.        ],
#        [-4.03887361, 10.        ]])

# In[simple MLP]

def round_min_zero(nums):
    nums = np.around(nums).astype(int)
    return np.maximum(0,nums)

def check_smape(y_pred, y_true):
    summ = np.maximum(np.abs(y_pred) + np.abs(y_true) + 0.1, 0.6)
    smape = np.mean(np.abs(y_pred-y_true) / summ *2)
    return smape*100

def simple_MLP(data, input_dim, name, deep=False):
    X_train, X_test, Y_train, Y_test = train_test_split(data.iloc[:,5:], data.iloc[:,1:5], test_size=0.2)
    
    # Create model
    model = Sequential()
    if deep:
        model.add(layers.Dense(512, input_dim=input_dim, activation='relu'))
        model.add(layers.Dense(256, activation='relu'))
        input_dim=256
    model.add(layers.Dense(128, input_dim=input_dim, activation='relu'))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(32, activation='relu'))
    model.add(layers.Dense(4, activation='linear'))
    
    # model.add(layers.Dense(64, input_dim=input_dim, activation='relu'))
    # model.add(layers.Dense(64, activation='relu'))
    # model.add(layers.Dense(64, activation='relu'))
    # model.add(layers.Dense(32, activation='relu'))
    # model.add(layers.Dense(16, activation='relu'))
    # model.add(layers.Dense(4, activation='linear'))
    

    # Compile the model
    model.compile(loss=SMAPE, optimizer='adam')

    # Train the model and track the history
    history = model.fit(X_train, Y_train, epochs=100, batch_size=32)

    # Evaluate the model on the testing set
    loss = model.evaluate(X_test, Y_test)

    # Use the model to make predictions on the testing set
    predictions = model.predict(X_test)
    # print(predictions)
    rounded = np.around(predictions)
    
    # Plot the training loss as a function of epoch
    plt.plot(history.history['loss'])
    plt.title(f'{name} Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('SMAPE Loss')
    plt.show()
    
    return [loss, check_smape(rounded,Y_test.values), check_smape(round_min_zero(predictions), Y_test.values)]
    
# In[prot+pep merge]
merge_updrs_protein_pivot = pd.merge(df_updrs_protein.iloc[:,:5], 
                                     proteins_lr_mean, 
                                     on=['visit_id'])
updrs_protein_lr_mean = merge_updrs_protein_pivot.drop_duplicates()
updrs_protein_lr_mean.dropna(inplace=True)

for col in updrs_protein_lr_mean.iloc[:,1:5].columns:
    print(f'{col}: {updrs_protein_lr_mean[col].isna().sum()}')
    
merge_updrs_protein_pivot = pd.merge(df_updrs_protein.iloc[:,:5], 
                                     proteins_lr_knn, 
                                     on=['visit_id'])
updrs_protein_lr_knn = merge_updrs_protein_pivot.drop_duplicates()
updrs_protein_lr_knn.dropna(inplace=True)

for col in updrs_protein_lr_knn.iloc[:,1:5].columns:
    print(f'{col}: {updrs_protein_lr_knn[col].isna().sum()}')
    
merge_updrs_protein_pivot = pd.merge(df_updrs_protein.iloc[:,:5], 
                                     proteins_lr_zero, 
                                     on=['visit_id'])
updrs_protein_lr_zero = merge_updrs_protein_pivot.drop_duplicates()
updrs_protein_lr_zero.dropna(inplace=True)

for col in updrs_protein_lr_mean.iloc[:,1:5].columns:
    print(f'{col}: {updrs_protein_lr_zero[col].isna().sum()}')
    
merge_updrs_protein_pivot = pd.merge(df_updrs_protein.iloc[:,:5], 
                                     proteins_knn, 
                                     on=['visit_id'])
updrs_protein_knn = merge_updrs_protein_pivot.drop_duplicates()
updrs_protein_knn.dropna(inplace=True)

for col in updrs_protein_knn.iloc[:,1:5].columns:
    print(f'{col}: {updrs_protein_knn[col].isna().sum()}')
    
merge_updrs_protein_pivot = pd.merge(df_updrs_protein.iloc[:,:5], 
                                     proteins_mean, 
                                     on=['visit_id'])
updrs_protein_mean = merge_updrs_protein_pivot.drop_duplicates()
updrs_protein_mean.dropna(inplace=True)

for col in updrs_protein_knn.iloc[:,1:5].columns:
    print(f'{col}: {updrs_protein_knn[col].isna().sum()}')

merge_updrs_protein_pivot = pd.merge(df_updrs_protein.iloc[:,:5], 
                                     proteins_zero, 
                                     on=['visit_id'])
updrs_protein_zero = merge_updrs_protein_pivot.drop_duplicates()
updrs_protein_zero.dropna(inplace=True)

for col in updrs_protein_zero.iloc[:,1:5].columns:
    print(f'{col}: {updrs_protein_zero[col].isna().sum()}')



# In[prep peptide data]
merge_updrs_peptide_pivot = pd.merge(df_updrs_peptide.iloc[:,:5], 
                                     peptides_lr_knn, 
                                     on=['visit_id'])
updrs_peptide_lr_knn = merge_updrs_peptide_pivot.drop_duplicates()
updrs_peptide_lr_knn.dropna(inplace=True)

for col in updrs_peptide_lr_knn.iloc[:,1:5].columns:
    print(f'{col}: {updrs_peptide_lr_knn[col].isna().sum()}')
    
merge_updrs_peptide_pivot = pd.merge(df_updrs_peptide.iloc[:,:5], 
                                     peptides_lr_mean, 
                                     on=['visit_id'])
updrs_peptide_lr_mean = merge_updrs_peptide_pivot.drop_duplicates()
updrs_peptide_lr_mean.dropna(inplace=True)

for col in updrs_peptide_lr_mean.iloc[:,1:5].columns:
    print(f'{col}: {updrs_peptide_lr_mean[col].isna().sum()}')

merge_updrs_peptide_pivot = pd.merge(df_updrs_peptide.iloc[:,:5], 
                                     peptides_lr_zero, 
                                     on=['visit_id'])
updrs_peptide_lr_zero = merge_updrs_peptide_pivot.drop_duplicates()
updrs_peptide_lr_zero.dropna(inplace=True)

for col in updrs_peptide_lr_zero.iloc[:,1:5].columns:
    print(f'{col}: {updrs_peptide_lr_zero[col].isna().sum()}')

merge_updrs_peptide_pivot = pd.merge(df_updrs_peptide.iloc[:,:5], 
                                     peptides_knn, 
                                     on=['visit_id'])
updrs_peptide_knn = merge_updrs_peptide_pivot.drop_duplicates()
updrs_peptide_knn.dropna(inplace=True)

for col in updrs_peptide_knn.iloc[:,1:5].columns:
    print(f'{col}: {updrs_peptide_knn[col].isna().sum()}')
    
merge_updrs_peptide_pivot = pd.merge(df_updrs_peptide.iloc[:,:5], 
                                     peptides_mean, 
                                     on=['visit_id'])
updrs_peptide_mean = merge_updrs_peptide_pivot.drop_duplicates()
updrs_peptide_mean.dropna(inplace=True)

for col in updrs_peptide_mean.iloc[:,1:5].columns:
    print(f'{col}: {updrs_peptide_mean[col].isna().sum()}')
    
merge_updrs_peptide_pivot = pd.merge(df_updrs_peptide.iloc[:,:5], 
                                     peptides_zero, 
                                     on=['visit_id'])
updrs_peptide_zero = merge_updrs_peptide_pivot.drop_duplicates()
updrs_peptide_zero.dropna(inplace=True)

for col in updrs_peptide_zero.iloc[:,1:5].columns:
    print(f'{col}: {updrs_peptide_zero[col].isna().sum()}')

# In[prep merged data]

merged_all_lr_knn = pd.merge(updrs_protein_lr_knn, peptides_lr_knn, 
                      on=['visit_id'])

merged_all_lr_mean = pd.merge(updrs_protein_lr_mean, peptides_lr_mean, on=['visit_id'])

merged_all_lr_zero = pd.merge(updrs_protein_lr_zero, peptides_lr_zero, on=['visit_id'])

merged_all_knn = pd.merge(updrs_protein_knn, peptides_knn, on=['visit_id'])

merged_all_mean = pd.merge(updrs_protein_mean, peptides_mean, on=['visit_id'])

merged_all_zero = pd.merge(updrs_protein_zero, peptides_zero, on=['visit_id'])



# In[run models]
loss_prot = [[],[],[],[],[],[]]
loss_pep = [[],[],[],[],[],[]]
loss_merge = [[],[],[],[],[],[]]

x=0
while x<5:
    print('************************************************************', x)
    loss_prot[0].append(simple_MLP(updrs_protein_lr_knn, 227, 'Proteins: LR + KNN'))
    loss_prot[1].append(simple_MLP(updrs_protein_lr_mean, 227, 'Proteins: LR + Mean'))
    loss_prot[2].append(simple_MLP(updrs_protein_lr_zero, 227, 'Proteins: LR + Zero'))
    loss_prot[3].append(simple_MLP(updrs_protein_knn, 227, 'Proteins: KNN'))
    loss_prot[4].append(simple_MLP(updrs_protein_mean, 227, 'Proteins: Mean'))
    loss_prot[5].append(simple_MLP(updrs_protein_zero, 227, 'Proteins: Zero'))
    
    loss_pep[0].append(simple_MLP(updrs_peptide_lr_knn, 968, 'Peptides: LR + KNN', True))
    loss_pep[1].append(simple_MLP(updrs_peptide_lr_mean, 968,'Peptides: LR + Mean', True))
    loss_pep[2].append(simple_MLP(updrs_peptide_lr_zero, 968, 'Peptides: LR + Zero', True))
    loss_pep[3].append(simple_MLP(updrs_peptide_knn, 968, 'Peptides: KNN', True))
    loss_pep[4].append(simple_MLP(updrs_peptide_mean, 968, 'Peptides: Mean', True))
    loss_pep[5].append(simple_MLP(updrs_peptide_zero, 968, 'Peptides: Zero', True))
    
    loss_merge[0].append(simple_MLP(merged_all_lr_knn, 1195, 'Merged: LR + KNN', True))
    loss_merge[1].append(simple_MLP(merged_all_lr_mean, 1195, 'Merged: LR + Mean', True))
    loss_merge[2].append(simple_MLP(merged_all_lr_zero, 1195, 'Merged: LR + Zero', True))
    loss_merge[3].append(simple_MLP(merged_all_knn, 1195, 'Merged: KNN', True))
    loss_merge[4].append(simple_MLP(merged_all_mean, 1195, 'Merged: Mean', True))
    loss_merge[5].append(simple_MLP(merged_all_zero, 1195, 'Merged: Zero', True))
    
    x+=1


# In[results!!!!!!]

lprot = [np.mean(x, axis=0) for x in loss_prot]
lpep = [np.mean(x, axis=0) for x in loss_pep]
lmerge = [np.mean(x, axis=0) for x in loss_merge]

outputs = [lprot, lpep, lmerge]
x_vals = np.arange(len(lprot))
width = 0.25
names = ['LR + KNN', 'LR + Mean', 'LR + Zero Fill', 'KNN', 'Mean', 'Zero Fill']
plot_titles = ['Proteins', 'Peptides', 'Merged']
loss_type = ['Floats', 'Rounded', 'Round Min 0']

fig,axes = plt.subplots(nrows=3, ncols=1, figsize=(10,10), layout='tight')
for i,axis in enumerate(axes):
    data = list(zip(*outputs[i]))
    axis.bar(x_vals - width, data[0], width, label=loss_type[0])
    axis.bar(x_vals, data[1], width, label=loss_type[1])
    axis.bar(x_vals + width, data[2], width, label=loss_type[2])
    axis.set_xlabel('Imputation Method')
    axis.set_ylabel('Avg. SMAPE Loss')
    axis.set_xticks(x_vals)
    axis.set_xticklabels(names)
    axis.set_ylim(40,90)
    axis.set_title(plot_titles[i])
    axis.legend(loc='upper right')
fig.suptitle('Test SMAPE Scores with Various Train Data Structures and Imputation Methods')
print(f'Minimum Average SMAPE score: {np.min(outputs, axis=1)} at {np.argmin(outputs, axis=1)}')

# In[another figure]
x_vals = np.arange(2)
fig,axes = plt.subplots(nrows=3, ncols=1, figsize=(10,10), layout='tight')
for i,axis in enumerate(axes):
    data = list(zip(*outputs[i]))
    for j in range(3):
        axis.bar(x_vals -0.25 + j*width, [data[2][j], data[2][j+3]], width, label=names[j+3])
    axis.set_ylabel('Avg. SMAPE Loss')
    axis.set_xticks(x_vals)
    axis.set_xticklabels(['LR', 'Control'])
    axis.set_ylim(40,90)
    axis.set_title(plot_titles[i])
    axis.legend(loc='upper right')
fig.suptitle('Test SMAPE Scores with Various Train Data Structures and Imputation Methods')
    
# In[PCA MLP]


pca_updrs_prot = list()
pca_updrs_pep = list()
pca_merged = list()
for name in names:
    df_pca_prot = pd.DataFrame(pca_prot_data[name][1], index = df_indices)
    df_updrs_pca_prot = pd.merge(df_updrs_protein.iloc[:,:5],
                                 df_pca_prot, 
                                 on='visit_id')
    df_updrs_pca_prot.drop_duplicates(inplace=True)
    df_updrs_pca_prot.dropna(inplace=True)
    pca_updrs_prot.append(df_updrs_pca_prot)
    
    df_pca_pep = pd.DataFrame(pca_pep_data[name][1], index = df_indices)
    df_updrs_pca_pep = pd.merge(df_updrs_peptide.iloc[:,:5],
                                df_pca_pep,
                                on='visit_id')
    df_updrs_pca_pep.drop_duplicates(inplace=True)
    df_updrs_pca_pep.dropna(inplace=True)
    pca_updrs_pep.append(df_updrs_pca_pep)
    
    pca_merged.append(pd.merge(pca_updrs_prot[-1], df_pca_pep, on='visit_id'))
    
    
# In[pca mlps]
results = [[[],[],[],[],[],[]],[[],[],[],[],[],[]],[[],[],[],[],[],[]]]
x = 0
while x < 5:
    for i,data in enumerate([pca_updrs_prot, pca_updrs_pep, pca_merged]):
        for j,imp_type in enumerate(names):
            results[i][j].append(simple_MLP(data[j], len(data[j].columns)-5, plot_titles[i]+': '+imp_type))
    x+=1
# In[PCA MLP results]

res_avg = [[np.mean(x,axis=0) for x in res]for res in results]

x_vals = np.arange(len(res_avg[0]))
width = 0.25
names = ['LR + KNN', 'LR + Mean', 'LR + Zero Fill', 'KNN', 'Mean', 'Zero Fill']
plot_titles = ['Proteins', 'Peptides', 'Merged']
loss_type = ['Floats', 'Rounded', 'Round Min 0']

fig,axes = plt.subplots(nrows=3, ncols=1, figsize=(10,10), layout='tight')
for i,axis in enumerate(axes):
    data = list(zip(*res_avg[i]))
    axis.bar(x_vals - width, data[0], width, label=loss_type[0])
    axis.bar(x_vals, data[1], width, label=loss_type[1])
    axis.bar(x_vals + width, data[2], width, label=loss_type[2])
    axis.set_xlabel('Imputation Method')
    axis.set_ylabel('Avg. SMAPE Loss')
    axis.set_xticks(x_vals)
    axis.set_xticklabels(names)
    axis.set_ylim(40,90)
    axis.set_title(plot_titles[i])
    axis.legend(loc='upper right')
fig.suptitle('Test SMAPE Scores with Various Train PCA Data Structures and Imputation Methods')
print(f'Minimum Average SMAPE score: {np.min(res_avg)}')

# In[another figure]
x_vals = np.arange(2)
fig,axes = plt.subplots(nrows=3, ncols=1, figsize=(10,10), layout='tight')
for i,axis in enumerate(axes):
    data = list(zip(*res_avg[i]))
    for j in range(3):
        axis.bar(x_vals -0.25 + j*width, [data[2][j], data[2][j+3]], width, label=names[j+3])
    axis.set_ylabel('Avg. SMAPE Loss')
    axis.set_xticks(x_vals)
    axis.set_xticklabels(['LR', 'Control'])
    axis.set_ylim(40,90)
    axis.set_title(plot_titles[i])
    axis.legend(loc='upper right')
fig.suptitle('Test SMAPE Scores with Various Train PCA Data Structures and Imputation Methods')
