# -*- coding: utf-8 -*-
"""
Created on Wed Apr  5 12:17:43 2023

@author: matt
"""


# In[0]


import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import KNNImputer, SimpleImputer
from scipy import linalg
import collections
import matplotlib.pyplot as plt

df_train_clinical = pd.read_csv('C:/Users/matt.DESKTOP-MPIK5VI/Desktop/ECE 539/Project/Data/train_clinical_data.csv')
df_train_peptides = pd.read_csv('C:/Users/matt.DESKTOP-MPIK5VI/Desktop/ECE 539/Project/Data/train_peptides.csv')
df_train_proteins = pd.read_csv('C:/Users/matt.DESKTOP-MPIK5VI/Desktop/ECE 539/Project/Data/train_proteins.csv')

# In[1]


print(f'Unique Clinical Data patient count: {df_train_clinical["patient_id"].nunique()}')
print('-------------------------------------------------------')
print('Null Values Found in Clinical Data:')
for col in df_train_clinical.columns:
    print(f'Null values found in {col}: {df_train_clinical[col].isna().sum()}')

print(f'\nUnique Peptide Data patient count: {df_train_peptides["patient_id"].nunique()}')
print(f'Unique Peptide count: {df_train_peptides["Peptide"].nunique()}')
print(f'Unique Protein count: {df_train_peptides["UniProt"].nunique()}')
print('-------------------------------------------------------')
print('Null Values Found in Peptide Data:')
for col in df_train_peptides.columns:
    print(f'Null values found in {col}: {df_train_peptides[col].isna().sum()}')
    
print(f'\nUnique Protein Data patient count: {df_train_proteins["patient_id"].nunique()}')
print(f'Unique Protein count: {df_train_proteins["UniProt"].nunique()}')
print('-------------------------------------------------------')
print('Null Values Found in Protein Data:')
for col in df_train_proteins.columns:
    print(f'Null values found in {col}: {df_train_proteins[col].isna().sum()}')
    
print('\nInput Data Shapes:')
print(f'Clinical Data: {df_train_clinical.shape}')
print(f'Peptide Data: {df_train_peptides.shape}')
print(f'Protein Data: {df_train_proteins.shape}')

# In[2]


def SMAPE(y_true, y_pred):
    '''
    Calculate percentage error(pe), pe initialized to zeros so division by 0 
    - when y_true = y_pred = 0 - can be avoided. Calculate mean and change to 
    percentage before returning.

    Parameters
    ----------
    y_true : array
        True target values.
    y_pred : array
        predicted target values.

    Returns
    -------
    float
        SMAPE of true vs predicted values.

    '''
    pe = np.zeros(len(y_true))
    num = np.abs(y_true-y_pred)
    den = (np.abs(y_true)+np.abs(y_pred))/2
    
    pos_ind = (y_true != 0)|(y_pred != 0)
    pe[pos_ind] = num[pos_ind]/den(pos_ind)
    
    return 100*np.mean(pe)

# In[3]


# Restructure to have UniProt columns, visit_id rows and NPX as values
protein_pivot = df_train_proteins.pivot(index = 'visit_id', columns=['UniProt'], values='NPX')
print(protein_pivot.head(10))

# Check if NaN's in protein data are due to missing peptide abundances 
dict_visit_uniprot_pep = collections.defaultdict(set)
for visit, uniprot in zip(df_train_peptides['visit_id'].values, df_train_peptides['UniProt'].values):
    dict_visit_uniprot_pep[visit].add(uniprot)

dict_visit_uniprot_prot = collections.defaultdict(set)
for visit, uniprot in zip(df_train_proteins['visit_id'].values, df_train_proteins['UniProt'].values):
    dict_visit_uniprot_prot[visit].add(uniprot)

# If true, then each visit contains the same set of proteins in both train_peptides and train_proteins
print(dict_visit_uniprot_pep==dict_visit_uniprot_prot)
# Cannot impute missing NPX values from PeptideAbundance

# In[Test with columntransformer]



# In[4]


# Imputation
# KNN
imputer_prot_knn = KNNImputer(missing_values=np.nan) # default: k=5
print('NaN before knn imputation:', protein_pivot.isna().sum().sum())
proteins_knn = imputer_prot_knn.fit_transform(protein_pivot)
print('NaN after knn imputation:', np.sum(np.isnan(proteins_knn)))

# Mean
imputer_prot_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before mean imputation:', protein_pivot.isna().sum().sum())
proteins_mean = imputer_prot_mean.fit_transform(protein_pivot)
print('NaN after mean imputation:', np.sum(np.isnan(proteins_mean)))

# Zero
print('NaN before 0 imputation:', protein_pivot.isna().sum().sum())
proteins_zero = protein_pivot.fillna(0, inplace=False)
print('NaN after 0 imputation:', proteins_zero.isna().sum().sum())

# In[4.5] Prep for Linear Regression


def visit_id_to_patient_month(visit_id):
    '''
    Takes an array of visit_ids, converts to list of lists of shape=(len(visit_id),2)
    where index 0 contains visit_ids and index 1 contains visit_months
    '''
    return [np.array(x.split('_'), dtype='int32') for x in visit_id]

def patient_month_to_visit_id(patient, month):
    return '_'.join(np.array([patient,month], dtype='str'))

# Store visit_month and NPX pairs for each protein for each patient in dictionary
# (patient_id, UniProt):[(visit_month, NPX), ...]
dict_patient_NPX = {key:[g['visit_month'].values, g['NPX'].values] 
                    for key,g in df_train_proteins.groupby(['patient_id', 'UniProt'])}



# In[4.75]


# Create an imputer that predicts missing peptide/protein values via linear regression
# of a patients visits that successfully reported the PeptideAbundance/NPX of the 
# peptide/protein of interest.

def linRegression(X, Y, month):
    '''
    Performs linear regression via least squares to estimate the coefficients for
    the line of best fit between X and Y data. The coefficients are then used to
    estimate the value associated with that of the month parameter.

    Parameters
    ----------
    X : array
        X-values or independent variables to use for linear regression
        Shape=(n,)
    Y : array
        Y values or dependent variable to use for linear regression
        Shape=(n,)
    month : int
        The value of the independent variable for which the corresponding predicted
        dependent value is desired.

    Returns
    -------
    float
        The predicted value calculated via month and the linear regression coefficients

    '''
    #If only one other visit has an NPX value, return that value
    if len(Y) == 1:
        return Y[0]
    
    # prep data
    poly = np.zeros((len(X),2))
    for i,num in enumerate(X):
        for j in range(2):
            poly[i][j] = num**(j)       
    
    # Calculate coefficients and return predicted NPX
    poly_t = np.transpose(poly)
    coeffs = ((linalg.inv(poly_t.dot(poly)).dot(poly_t))).dot(Y)
    return np.array([1,month]).dot(coeffs)


# In[linear regression imputation]

# Implement imputation via linear regression
def LR_Imputer(data, data_dict):
    '''
    Applies linear regression as a strategy for imputation of data with NaN values

    Parameters
    ----------
    data : DataFrame
        Indices: visit_id
        Columns: Either peptide names or UniProt ids
    data_dict : dict
        Keys: (patient_id, UniProt) tuple
        Values: [visit_month, NPX or PeptideAbundance] list of np.arrays of int32s

    Returns
    -------
    New DataFrame object that is result of imputation

    '''
    print('NaN before linear regression imputation:', data.isna().sum().sum())
    
    data_copy = data.copy(deep=True)
    # Iterate through all columns (protein/peptide) in data
    for name, content in data_copy.items():
        # If no NaN's for this protein, jump to next
        if content.isna().sum() == 0:
            continue
        keys = visit_id_to_patient_month(content.index)
        skip_patient = None
        
        # Iterate over each row (visit_id) for protein
        for idx,npx in enumerate(content):
            patient = keys[idx][0]
            
            if np.isnan(npx) and (patient != skip_patient):
                if (patient,name) not in data_dict.keys():
                    skip_patient = patient
                    continue
                X, Y = data_dict[(patient, name)]
                visit_id = patient_month_to_visit_id(patient, keys[idx][1])
                pred_npx = linRegression(X, Y, keys[idx][1])
                data_copy.loc[visit_id, name] = pred_npx
    
    print('NaN after linear regression imputation:', data_copy.isna().sum().sum())
    return data_copy

proteins_lr = LR_Imputer(protein_pivot, dict_patient_NPX)

# Assess imputation
has_null = 0
for col in proteins_lr:
    if proteins_lr[col].isna().sum() != 0:
        has_null+=1
print(f'Percent proteins with >= one null value after imputation: {100*has_null/len(proteins_lr.columns)}')
has_null = 0
for idx in proteins_lr.index:
    if proteins_lr.loc[idx].isna().sum() != 0:
        has_null+=1
print(f'Percent visits with >= one null value after imputation: {100*has_null/len(proteins_lr.index)}')

# In[Assess imputation]

# Still has null values, can set to 0, use full column means, or knn
# KNN
imputer_prot_lr_knn = KNNImputer(missing_values=np.nan) # default: k=5
print('NaN before knn imputation:', proteins_lr.isna().sum().sum())
proteins_lr_knn = imputer_prot_knn.fit_transform(proteins_lr)
print('NaN after knn imputation:', np.sum(np.isnan(proteins_lr_knn)))

# Mean
imputer_prot_lr_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before mean imputation:', proteins_lr.isna().sum().sum())
proteins_lr_mean = imputer_prot_mean.fit_transform(proteins_lr)
print('NaN after mean imputation:', np.sum(np.isnan(proteins_lr_mean)))

# Zero
print('NaN before 0 imputation:', proteins_lr.isna().sum().sum())
proteins_lr_zero = proteins_lr.fillna(0, inplace=False)
print('NaN after 0 imputation:', proteins_lr_zero.isna().sum().sum())
# In[5]

# Scale Protein NPX values for all imputation methods
scaler_proteins_knn = StandardScaler()
proteins_knn = scaler_proteins_knn.fit_transform(proteins_knn)

scaler_proteins_mean = StandardScaler()
proteins_mean = scaler_proteins_mean.fit_transform(proteins_mean)

scaler_proteins_zero = StandardScaler()
proteins_zero = scaler_proteins_zero.fit_transform(proteins_zero)

scaler_proteins_lr_knn = StandardScaler()
proteins_lr_knn = scaler_proteins_lr_knn.fit_transform(proteins_lr_knn)

scaler_proteins_lr_mean = StandardScaler()
proteins_lr_mean = scaler_proteins_lr_mean.fit_transform(proteins_lr_mean)

scaler_proteins_lr_zero = StandardScaler()
proteins_lr_zero = scaler_proteins_lr_zero.fit_transform(proteins_lr_zero)

# In[6]


# PCA on proteins for all imputation methods
def run_PCA(data, names, explained_var):
    '''
    Performs PCA on each array in data.

    Parameters
    ----------
    data : list of numpy arrays
        Datasets containing protein or peptide abundance data at each patient visit
    names : list of strings
        Names of each dataset, index should match that of the corresponding dataset's in data
    explained_var : float
        Between zero and one, desired ratio of explained variance. The PCA objects
        use this to determine how many principal components to compute.

    Returns
    -------
    dict
        keys : int
            Names of original data from the argument names
        values : list
            Index 0 contains the PCA object fit to the data
            Index 1 contains the transformed data as a numpy array

    '''
    pca_objs = []
    pca_res = []
    for array in data:
        pca_objs.append(PCA(explained_var, svd_solver='full'))
        pca_res.append(pca_objs[-1].fit_transform(array))

    return {name:(obj,res) for name,obj,res in zip(names,pca_objs,pca_res)}

# Prep data for PCA function
scaled_prot_data = [proteins_knn, proteins_mean, proteins_zero,
               proteins_lr_knn, proteins_lr_mean, proteins_lr_zero]
scaled_names = ['KNN', 'Mean', 'Zero Fill', 'LR + KNN', 'LR + Mean', 'LR + Zero Fill']

# Perform PCA
pca_prot_data = run_PCA(scaled_prot_data, scaled_names, 0.85)


# In[7]

def Visualize_PCA_Results(data, explained_var, sup_title=''):
    '''
    Visualize the results of PCA on six different types of data. Figure contains six
    scatter plots and one bar graph along the bottom. Each scatter plot shows the 
    first two Principal components of the specified PCA data. The bar graph plots the
    explained variance ratios of the first 10 PC's for each of the 6 datasets. 
    The function also prints the number of PC's required to reach the desired explained
    variance ratio.

    Parameters
    ----------
    data : dict
        Output of run_PCA function:
        keys : int
            Dataset names
        values : list
            Index 0 contains the PCA object fit to the data
            Index 1 contains the transformed data as a numpy array
    explained_var : float
        Between zero and one, desired ratio of explained variance. The PCA objects
        use this to determine how many principal components to compute.
    sup_title : str
        Allows customization of the figure's title to include the name of the data.
        For our use, either 'Protein' or 'Peptide'

    Returns
    -------
    None.

    '''
    width = 0.15
    sing_values = 10
    x_vals = np.arange(sing_values)
    titles = data.keys()
    
    fig,axes = plt.subplots(ncols=3, nrows=3, figsize=(10,10), layout='tight')
    fig.suptitle(f'PCA After {sup_title} Data Imputation')
    
    # Make large subplot on bottom row for combined bar graph
    gs = axes[2,0].get_gridspec()
    for ax in axes[2,:]:
        ax.remove()
    ax_bar = fig.add_subplot(gs[2,:])
    
    for i,title in enumerate(titles):
        # Build bar graph
        ax_bar.bar(x_vals-0.425+i*width, data[title][0].explained_variance_ratio_[:sing_values], width, label=title)
        ax_bar.set_xlabel('Principle Components')
        ax_bar.set_ylabel('Explained Variance Ratio')
        ax_bar.set_xticks(x_vals)
        ax_bar.set_xticklabels(['PC '+str(x+1) for x in x_vals])
        ax_bar.legend()
        
        # Add scatter plot of first two principal components
        cur_ax = axes[i//3, i%3]
        cur_ax.scatter(data[title][1][:,0], data[title][1][:,1])
        cur_ax.set_xlabel('PC 1')
        cur_ax.set_ylabel('PC 2')
        cur_ax.set_title(title)
    
    plt.tight_layout()
    
    print(f'PC\'s necessary to explain {explained_var*100}% of variance:')
    for title in titles:
        print(f'{title} Imputation: {data[title][0].n_components_}')

Visualize_PCA_Results(pca_prot_data, 0.85, 'Protein')
    
# In[8]


# Repeat analysis for peptides
peptide_pivot = df_train_peptides.pivot(index='visit_id', columns=['Peptide'], values='PeptideAbundance')
print(peptide_pivot.head(10))

# Imputation
imputer_pep_knn = KNNImputer(missing_values=np.nan)
print('NaN before KNN imputation (peptides): ', peptide_pivot.isna().sum().sum())
peptides_knn = imputer_pep_knn.fit_transform(peptide_pivot)
print('NaN after KNN imputation (peptides): ', np.sum(np.isnan(peptides_knn)))

imputer_pep_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before mean imputation (peptides): ', peptide_pivot.isna().sum().sum())
peptides_mean = imputer_pep_mean.fit_transform(peptide_pivot)
print('NaN after mean imputation (peptides): ', np.sum(np.isnan(peptides_mean)))

print('NaN before zero fill imputation (peptides): ', peptide_pivot.isna().sum().sum())
peptides_zero = peptide_pivot.fillna(0, inplace=False)
print('NaN after zero fill imputation (peptides): ', peptides_zero.isna().sum().sum())

dict_patient_Abund = {key:[g['visit_month'].values, g['PeptideAbundance'].values]
                      for key, g in df_train_peptides.groupby(['patient_id', 'Peptide'])}

peptides_lr = LR_Imputer(peptide_pivot, dict_patient_Abund)

# Assess imputation
has_null = 0
for col in peptides_lr:
    if peptides_lr[col].isna().sum() != 0:
        has_null+=1
print(f'Percent peptides with >= one null value after imputation: {100*has_null/len(peptides_lr.columns)}')
has_null = 0
for idx in peptides_lr.index:
    if peptides_lr.loc[idx].isna().sum() != 0:
        has_null+=1
print(f'Percent visits with >= one null value after imputation: {100*has_null/len(peptides_lr.index)}')

# Still has null values, can set to 0, use full column means, or knn
imputer_pep_lr_knn = KNNImputer(missing_values=np.nan)
print('NaN before KNN Imputation: ', peptide_pivot.isna().sum().sum())
peptides_lr_knn = imputer_pep_lr_knn.fit_transform(peptides_lr)
print('NaN after KNN Imputation: ', np.sum(np.isnan(peptides_lr_knn)))

imputer_pep_lr_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
print('NaN before Mean Imputation: ', peptide_pivot.isna().sum().sum())
peptides_lr_mean = imputer_pep_lr_mean.fit_transform(peptides_lr)
print('NaN after Mean Imputation: ', np.sum(np.isnan(peptides_lr_mean)))

print('NaN before Zero Fill Imputation: ', peptide_pivot.isna().sum().sum())
peptides_lr_zero = peptide_pivot.fillna(0, inplace=False)
print('NaN after Zero Fill Imputation: ', peptides_lr_zero.isna().sum().sum())


# Scaling
scaler_pep_knn = StandardScaler()
peptides_knn = scaler_pep_knn.fit_transform(peptides_knn)

scaler_pep_mean = StandardScaler()
peptides_mean = scaler_pep_mean.fit_transform(peptides_mean)

scaler_pep_zero = StandardScaler()
peptides_zero = scaler_pep_zero.fit_transform(peptides_zero)

scaler_pep_lr_knn = StandardScaler()
peptides_lr_knn = scaler_pep_lr_knn.fit_transform(peptides_lr_knn)

scaler_pep_lr_mean = StandardScaler()
peptides_lr_mean = scaler_pep_lr_mean.fit_transform(peptides_lr_mean)

scaler_pep_lr_zero = StandardScaler()
peptides_lr_zero = scaler_pep_lr_zero.fit_transform(peptides_lr_zero)

# PCA
scaled_pep_data = [peptides_knn, peptides_mean, peptides_zero,
                   peptides_lr_knn, peptides_lr_mean, peptides_lr_zero]

pca_pep_data = run_PCA(scaled_pep_data, scaled_names, 0.85)

# Plot PCA data
Visualize_PCA_Results(pca_pep_data, 0.85, 'Peptide')

# In[9]


pca_knn_components = pd.DataFrame(pca_prot_data['KNN'][0].components_)
print(pca_knn_components.iloc[0,:].idxmax())
zlook = pca_prot_data['KNN'][1]


############# Keep a column seperate during PCA!!!! ######################

# featurizer = ColumnTransformer([('pca', PCA(n_components=1), ['c1', 'c2']),
# ...                                 ('preserve', 'passthrough', ['c_to_preserve'])])
# >>> featurizer.fit_transform(df)
# array([[ 4.03887361, -5.        ],
#        [ 1.3462912 , -3.        ],
#        [-1.3462912 ,  6.        ],
#        [-4.03887361, 10.        ]])











































